import requests
from bs4 import BeautifulSoup
import os
import time
import html2text
from urllib.parse import urljoin, urlparse

BASE_URL = "https://docs.tronlink.org/"
SAVE_DIR = "downloaded_docs"
IMG_DIR = os.path.join(SAVE_DIR, "images")
HEADERS = {"User-Agent": "Mozilla/5.0"}

# åˆå§‹åŒ– html2text è½¬æ¢å™¨
h = html2text.HTML2Text()
h.ignore_links = False
h.ignore_images = False
h.body_width = 0

os.makedirs(SAVE_DIR, exist_ok=True)
os.makedirs(IMG_DIR, exist_ok=True)

def scrape_page(url):
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"Failed to fetch {url}: {e}")
        return None

def download_image(img_url, page_name, index):
    parsed = urlparse(img_url)
    ext = os.path.splitext(parsed.path)[1] or ".jpg"
    filename = f"{page_name}_img_{index}{ext}"
    local_path = os.path.join("images", filename)
    full_path = os.path.join(SAVE_DIR, local_path)

    try:
        img_data = requests.get(img_url, headers=HEADERS).content
        with open(full_path, "wb") as f:
            f.write(img_data)
        return local_path  # è¿”å›ç”¨äºæ›¿æ¢ Markdown çš„ç›¸å¯¹è·¯å¾„
    except Exception as e:
        print(f"âŒ å›¾ç‰‡ä¸‹è½½å¤±è´¥ {img_url} ï¼š{e}")
        return img_url  # ä¿ç•™åŸè·¯å¾„ä½œä¸ºå…œåº•

# æŠ“å–é¦–é¡µ
index_page = scrape_page(BASE_URL)
if not index_page:
    exit()

soup = BeautifulSoup(index_page, "html.parser")
links = set()

# æ”¶é›†æ‰€æœ‰é¡µé¢é“¾æ¥
for link in soup.select('a.toclink'):
    href = link.get("href")
    if href and not href.startswith(("http", "#")):
        links.add(href)

# æŠ“å–æ‰€æœ‰é¡µé¢å†…å®¹ + å›¾ç‰‡
for path in links:
    url = urljoin(BASE_URL, path)
    print(f"ğŸ“„ ä¸‹è½½é¡µé¢ï¼š{url}")

    html = scrape_page(url)
    if not html:
        continue

    page_soup = BeautifulSoup(html, "html.parser")
    content = page_soup.select_one(".page-default-width")

    if not content:
        continue

    page_name = path.strip("/").replace("/", "_") or "index"

    # ä¸‹è½½é¡µé¢ä¸­æ‰€æœ‰å›¾ç‰‡å¹¶æ›¿æ¢ src
    img_tags = content.find_all("img")
    for idx, img in enumerate(img_tags):
        src = img.get("src")
        if not src:
            continue
        full_img_url = urljoin(url, src)
        local_path = download_image(full_img_url, page_name, idx)
        img["src"] = local_path  # æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„

    # è½¬ Markdown
    markdown_content = h.handle(str(content))

    # ä¿å­˜ä¸º .md æ–‡ä»¶
    filename = os.path.join(SAVE_DIR, f"{page_name}.md")
    with open(filename, "w", encoding="utf-8") as f:
        f.write(markdown_content)

    time.sleep(1)

print("âœ… æ‰€æœ‰é¡µé¢ä¸å›¾ç‰‡å·²å¯¼å‡ºå®Œæˆï¼")